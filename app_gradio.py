# -*- coding: utf-8 -*-
"""
TrialRAG Web Interface (Gradio)

ä¸€ä¸ªç¾è§‚çš„ Web ç•Œé¢ï¼Œç”¨äºä¸´åºŠè¯•éªŒåŒ¹é…

å®‰è£…ä¾èµ–ï¼š
pip install gradio --break-system-packages

è¿è¡Œï¼š
python app_gradio.py

ç„¶ååœ¨æµè§ˆå™¨æ‰“å¼€æ˜¾ç¤ºçš„ URLï¼ˆé€šå¸¸æ˜¯ http://127.0.0.1:7860ï¼‰
"""

import os
import json
import gradio as gr
from datetime import datetime
from typing import List, Dict, Any

# å¯¼å…¥ä¸»ç¨‹åºçš„æ ¸å¿ƒåŠŸèƒ½
import sys
sys.path.append(os.path.dirname(__file__))

from TrialRAG_V2 import (
    load_trials,
    get_bm25_instance,
    bm25_retrieve,
    llm_rerank,
    normalize_text
)
from openai import OpenAI


# ================= å…¨å±€é…ç½® =================

# é»˜è®¤é…ç½®
DEFAULT_DATA_DIR = r"D:\å®ä¹ \TrialGPT-China\cleaned_out_V4\clean_parts"
DEFAULT_MODEL = "deepseek-chat"

# å…¨å±€å˜é‡ï¼ˆç¼“å­˜ï¼‰
TRIALS_CACHE = None
BM25_CACHE = None
DATA_DIR_CACHE = None


# ================= æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================

def initialize_system(data_dir: str, progress=gr.Progress()):
    """åˆå§‹åŒ–ç³»ç»Ÿï¼šåŠ è½½æ•°æ®å’Œæ„å»ºç´¢å¼•"""
    global TRIALS_CACHE, BM25_CACHE, DATA_DIR_CACHE
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½
    if DATA_DIR_CACHE == data_dir and TRIALS_CACHE is not None:
        return True, f"âœ… ç³»ç»Ÿå·²å°±ç»ªï¼ˆ{len(TRIALS_CACHE)} ä¸ªè¯•éªŒï¼‰"
    
    try:
        # åŠ è½½è¯•éªŒæ•°æ®
        progress(0.3, desc="åŠ è½½è¯•éªŒæ•°æ®...")
        trials = load_trials(data_dir, max_trials=0)
        
        if not trials:
            return False, f"âŒ é”™è¯¯ï¼šåœ¨ {data_dir} ä¸­æœªæ‰¾åˆ°è¯•éªŒæ•°æ®"
        
        # æ„å»º BM25 ç´¢å¼•
        progress(0.6, desc="æ„å»ºæ£€ç´¢ç´¢å¼•...")
        bm25 = get_bm25_instance(trials, cache_dir="./cache")
        
        # æ›´æ–°ç¼“å­˜
        TRIALS_CACHE = trials
        BM25_CACHE = bm25
        DATA_DIR_CACHE = data_dir
        
        progress(1.0, desc="åˆå§‹åŒ–å®Œæˆï¼")
        return True, f"âœ… ç³»ç»Ÿå°±ç»ªï¼šå·²åŠ è½½ {len(trials)} ä¸ªä¸´åºŠè¯•éªŒ"
        
    except Exception as e:
        return False, f"âŒ åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}"


def search_trials(
    patient_text: str,
    topk: int,
    topn: int,
    model: str,
    api_key: str,
    progress=gr.Progress()
):
    """æ‰§è¡Œä¸´åºŠè¯•éªŒåŒ¹é…"""
    
    # æ£€æŸ¥è¾“å…¥
    if not patient_text or not patient_text.strip():
        return "âŒ è¯·è¾“å…¥æ‚£è€…æè¿°", "", ""
    
    if not api_key or not api_key.strip():
        return "âŒ è¯·è®¾ç½® DeepSeek API Key", "", ""
    
    # æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦å·²åˆå§‹åŒ–
    if TRIALS_CACHE is None or BM25_CACHE is None:
        return "âŒ è¯·å…ˆç‚¹å‡»ã€Œåˆå§‹åŒ–ç³»ç»Ÿã€æŒ‰é’®", "", ""
    
    try:
        patient_text = normalize_text(patient_text)
        
        # Step 1: BM25 æ£€ç´¢
        progress(0.2, desc=f"BM25 æ£€ç´¢ä¸­ï¼ˆtopk={topk}ï¼‰...")
        candidates = bm25_retrieve(BM25_CACHE, TRIALS_CACHE, patient_text, topk)
        
        if not candidates:
            return "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³è¯•éªŒï¼Œè¯·æ£€æŸ¥æ‚£è€…æè¿°", "", ""
        
        # Step 2: LLM é‡æ’åº
        progress(0.4, desc=f"DeepSeek é‡æ’åºä¸­ï¼ˆ{len(candidates)} ä¸ªå€™é€‰ï¼‰...")
        
        client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
        
        # è¿›åº¦æ›´æ–°å›è°ƒ
        def update_progress(current, total):
            progress((0.4 + 0.5 * current / total), 
                    desc=f"åˆ†æä¸­ [{current}/{total}]...")
        
        ranked_results = llm_rerank_with_progress(
            client, model, patient_text, candidates, update_progress
        )
        
        progress(0.95, desc="ç”Ÿæˆç»“æœ...")
        
        # Step 3: æ ¼å¼åŒ–è¾“å‡º
        results_html = format_results_html(ranked_results[:topn])
        results_json = json.dumps({
            "timestamp": datetime.now().isoformat(),
            "patient_text": patient_text,
            "model": model,
            "topk": topk,
            "topn": topn,
            "results": ranked_results[:topn]
        }, ensure_ascii=False, indent=2)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = generate_statistics(ranked_results)
        
        progress(1.0, desc="å®Œæˆï¼")
        
        return results_html, results_json, stats
        
    except Exception as e:
        return f"âŒ åŒ¹é…å¤±è´¥ï¼š{str(e)}", "", ""


def llm_rerank_with_progress(client, model, patient_text, candidates, progress_callback):
    """å¸¦è¿›åº¦å›è°ƒçš„ LLM é‡æ’åº"""
    from TrialRAG_V2 import format_trial_for_llm, LLM_SYSTEM_PROMPT
    
    results = []
    total = len(candidates)
    
    for i, trial in enumerate(candidates, 1):
        try:
            trial_context = format_trial_for_llm(trial)
            
            user_prompt = f"""Patient Description:
{patient_text}

Clinical Trial:
{trial_context}

Provide your assessment in JSON format ONLY."""
            
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": LLM_SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.0,
                response_format={"type": "json_object"}
            )
            
            content = response.choices[0].message.content
            parsed = json.loads(content)
            
            results.append({
                "trial_id": trial.get("trial_id", ""),
                "title": trial.get("background", {}).get("public_title", ""),
                "label": parsed.get("label", "Insufficient"),
                "score": float(parsed.get("score", 0)),
                "reason": parsed.get("reason", ""),
                "evidence": parsed.get("evidence", ""),
            })
            
            # æ›´æ–°è¿›åº¦
            if progress_callback:
                progress_callback(i, total)
                
        except Exception as e:
            results.append({
                "trial_id": trial.get("trial_id", ""),
                "title": "",
                "label": "Error",
                "score": 0,
                "reason": f"å¤„ç†é”™è¯¯: {str(e)}",
                "evidence": "",
            })
    
    # æŒ‰åˆ†æ•°é™åºæ’åº
    results.sort(key=lambda x: x["score"], reverse=True)
    return results


def format_results_html(results: List[Dict[str, Any]]) -> str:
    """æ ¼å¼åŒ–ç»“æœä¸º HTML"""
    
    html = """
    <style>
        .result-container { font-family: Arial, sans-serif; }
        .result-card {
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 15px;
            margin-bottom: 15px;
            background: white;
        }
        .result-card.included { border-left: 4px solid #10b981; }
        .result-card.excluded { border-left: 4px solid #ef4444; }
        .result-card.insufficient { border-left: 4px solid #f59e0b; }
        .result-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 10px;
        }
        .result-label {
            padding: 4px 12px;
            border-radius: 4px;
            font-weight: bold;
            font-size: 14px;
        }
        .label-included { background: #d1fae5; color: #065f46; }
        .label-excluded { background: #fee2e2; color: #991b1b; }
        .label-insufficient { background: #fef3c7; color: #92400e; }
        .result-score {
            font-size: 24px;
            font-weight: bold;
            color: #1f2937;
        }
        .result-title { 
            font-size: 16px;
            font-weight: bold;
            color: #1f2937;
            margin-bottom: 8px;
        }
        .result-reason {
            color: #4b5563;
            margin-bottom: 8px;
            line-height: 1.6;
        }
        .result-evidence {
            background: #f9fafb;
            border-left: 3px solid #e5e7eb;
            padding: 10px;
            margin-top: 8px;
            font-size: 13px;
            color: #6b7280;
            font-style: italic;
        }
        .result-id {
            color: #9ca3af;
            font-size: 12px;
        }
    </style>
    <div class="result-container">
    """
    
    for i, result in enumerate(results, 1):
        label = result["label"]
        score = result["score"]
        
        # ç¡®å®šæ ·å¼ç±»
        card_class = label.lower() if label.lower() in ["included", "excluded", "insufficient"] else "insufficient"
        label_class = f"label-{card_class}"
        
        # æ ‡ç­¾å›¾æ ‡
        icon = {"included": "âœ…", "excluded": "âŒ", "insufficient": "âš ï¸"}.get(card_class, "â“")
        
        html += f"""
        <div class="result-card {card_class}">
            <div class="result-header">
                <span class="result-label {label_class}">{icon} {label}</span>
                <span class="result-score">{score:.1f}</span>
            </div>
            <div class="result-title">{i}. {result.get('title', 'æœªçŸ¥è¯•éªŒ')}</div>
            <div class="result-id">Trial ID: {result['trial_id']}</div>
            <div class="result-reason"><strong>ç†ç”±ï¼š</strong>{result['reason']}</div>
            <div class="result-evidence"><strong>å¼•ç”¨ï¼š</strong>"{result['evidence'][:200]}{'...' if len(result['evidence']) > 200 else ''}"</div>
        </div>
        """
    
    html += "</div>"
    return html


def generate_statistics(results: List[Dict[str, Any]]) -> str:
    """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
    label_counts = {}
    for r in results:
        label = r["label"]
        label_counts[label] = label_counts.get(label, 0) + 1
    
    stats = f"""
ğŸ“Š **åŒ¹é…ç»Ÿè®¡**

- æ€»å€™é€‰æ•°ï¼š{len(results)}
- âœ… Includedï¼š{label_counts.get('Included', 0)} ä¸ª
- âŒ Excludedï¼š{label_counts.get('Excluded', 0)} ä¸ª
- âš ï¸ Insufficientï¼š{label_counts.get('Insufficient', 0)} ä¸ª
- â“ Errorï¼š{label_counts.get('Error', 0)} ä¸ª

---

**å¹³å‡åˆ†æ•°ï¼š** {sum(r['score'] for r in results) / len(results):.2f}

**Top 5 åˆ†æ•°ï¼š** {', '.join([f"{r['score']:.1f}" for r in results[:5]])}
    """
    
    return stats


# ================= Gradio ç•Œé¢ =================

def create_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    with gr.Blocks(
        title="TrialRAG - ä¸´åºŠè¯•éªŒæ™ºèƒ½åŒ¹é…ç³»ç»Ÿ",
        theme=gr.themes.Soft()
    ) as demo:
        
        gr.Markdown("""
        # ğŸ¥ TrialRAG - ä¸´åºŠè¯•éªŒæ™ºèƒ½åŒ¹é…ç³»ç»Ÿ
        
        åŸºäº BM25 æ£€ç´¢ + DeepSeek å¤§æ¨¡å‹é‡æ’åºçš„ä¸´åºŠè¯•éªŒåŒ¹é…ç³»ç»Ÿ
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                # å·¦ä¾§ï¼šè¾“å…¥åŒºåŸŸ
                gr.Markdown("## ğŸ“ æ‚£è€…ä¿¡æ¯")
                
                patient_input = gr.Textbox(
                    label="æ‚£è€…æè¿°",
                    placeholder="""è¯·è¾“å…¥æ‚£è€…è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
- åŸºæœ¬ä¿¡æ¯ï¼šå¹´é¾„ã€æ€§åˆ«
- è¯Šæ–­ï¼šç–¾ç—…ç±»å‹ã€åˆ†æœŸ
- åˆ†å­æ ‡å¿—ç‰©ï¼šEGFRã€ALKã€HER2 ç­‰
- æ—¢å¾€æ²»ç–—å²
- ä½“èƒ½çŠ¶æ€ï¼šECOG è¯„åˆ†
- å®éªŒå®¤æ£€æŸ¥ï¼šè¡€å¸¸è§„ã€è‚è‚¾åŠŸèƒ½
- åˆå¹¶ç—‡

ç¤ºä¾‹ï¼š
æ‚£è€…ç”·æ€§62å²ï¼Œè¯Šæ–­ä¸ºæ™šæœŸéå°ç»†èƒè‚ºç™Œï¼Œä¸´åºŠåˆ†æœŸIVæœŸã€‚ç—…ç†ç±»å‹ä¸ºè‚ºè…ºç™Œã€‚EGFRåŸºå› 19å·å¤–æ˜¾å­ç¼ºå¤±çªå˜é˜³æ€§ï¼ŒALKèåˆåŸºå› é˜´æ€§ã€‚æ‚£è€…ä¸ºåˆè¯Šï¼Œå°šæœªæ¥å—ä»»ä½•ç³»ç»Ÿæ€§æŠ—è‚¿ç˜¤æ²»ç–—ã€‚ECOGä½“èƒ½çŠ¶æ€è¯„åˆ†1åˆ†ï¼Œä½“é‡65å…¬æ–¤ã€‚è¡€å¸¸è§„ã€è‚è‚¾åŠŸèƒ½æ­£å¸¸ã€‚æ— å…¶ä»–ä¸¥é‡ç–¾ç—…ã€‚""",
                    lines=15
                )
                
                gr.Markdown("## âš™ï¸ ç³»ç»Ÿé…ç½®")
                
                
                data_dir_input = gr.Textbox(
                    label="æ•°æ®ç›®å½•è·¯å¾„",
                    value=DEFAULT_DATA_DIR,
                    placeholder=r"D:\å®ä¹ \TrialGPT-China\cleaned_out_V4\clean_parts"
                )
                init_btn = gr.Button("ğŸ”„ åˆå§‹åŒ–ç³»ç»Ÿ", variant="secondary")
                init_status = gr.Markdown("ğŸ’¡ è¯·å…ˆåˆå§‹åŒ–ç³»ç»Ÿ")
                
                with gr.Accordion("æ£€ç´¢å‚æ•°", open=True):
                    topk_slider = gr.Slider(
                        minimum=10,
                        maximum=100,
                        value=30,
                        step=10,
                        label="BM25 å¬å›æ•°é‡ (topk)",
                        info="åˆæ­¥å¬å›çš„å€™é€‰è¯•éªŒæ•°é‡"
                    )
                    
                    topn_slider = gr.Slider(
                        minimum=5,
                        maximum=20,
                        value=10,
                        step=5,
                        label="æœ€ç»ˆæ¨èæ•°é‡ (topn)",
                        info="ç»è¿‡ LLM é‡æ’åºåå±•ç¤ºçš„æ•°é‡"
                    )
                
                with gr.Accordion("æ¨¡å‹è®¾ç½®", open=False):
                    model_input = gr.Textbox(
                        label="DeepSeek æ¨¡å‹",
                        value=DEFAULT_MODEL,
                        placeholder="deepseek-chat"
                    )
                    
                    api_key_input = gr.Textbox(
                        label="DeepSeek API Key",
                        type="password",
                        placeholder="sk-xxxx",
                        value=os.getenv("DEEPSEEK_API_KEY", "")
                    )
                
                search_btn = gr.Button("ğŸ” å¼€å§‹åŒ¹é…", variant="primary", size="lg")
            
            with gr.Column(scale=2):
                # å³ä¾§ï¼šç»“æœåŒºåŸŸ
                gr.Markdown("## ğŸ“Š åŒ¹é…ç»“æœ")
                
                with gr.Tabs():
                    with gr.Tab("å¯è§†åŒ–ç»“æœ"):
                        results_html = gr.HTML(label="åŒ¹é…ç»“æœ")
                    
                    with gr.Tab("JSON æ•°æ®"):
                        results_json = gr.Code(
                            label="JSON ç»“æœ",
                            language="json",
                            lines=20
                        )
                    
                    with gr.Tab("ç»Ÿè®¡ä¿¡æ¯"):
                        stats_output = gr.Markdown(label="ç»Ÿè®¡")
        
        # äº‹ä»¶ç»‘å®š
        init_btn.click(
            fn=initialize_system,
            inputs=[data_dir_input],
            outputs=[gr.State(), init_status]
        )
        
        search_btn.click(
            fn=search_trials,
            inputs=[
                patient_input,
                topk_slider,
                topn_slider,
                model_input,
                api_key_input
            ],
            outputs=[results_html, results_json, stats_output]
        )
        
        # ç¤ºä¾‹
        gr.Markdown("---")
        gr.Markdown("### ğŸ’¡ å¿«é€Ÿç¤ºä¾‹")
        
        gr.Examples(
            examples=[
                ["æ‚£è€…ç”·æ€§62å²ï¼Œè¯Šæ–­ä¸ºæ™šæœŸéå°ç»†èƒè‚ºç™Œï¼Œä¸´åºŠåˆ†æœŸIVæœŸã€‚ç—…ç†ç±»å‹ä¸ºè‚ºè…ºç™Œã€‚EGFRåŸºå› 19å·å¤–æ˜¾å­ç¼ºå¤±çªå˜é˜³æ€§ï¼ŒALKèåˆåŸºå› é˜´æ€§ï¼ŒPD-L1è¡¨è¾¾TPS 5%ã€‚æ‚£è€…ä¸ºåˆè¯Šï¼Œå°šæœªæ¥å—ä»»ä½•ç³»ç»Ÿæ€§æŠ—è‚¿ç˜¤æ²»ç–—ï¼Œæœªæ¥å—è¿‡æ”¾ç–—æˆ–æ‰‹æœ¯ã€‚ECOGä½“èƒ½çŠ¶æ€è¯„åˆ†1åˆ†ï¼Œä½“é‡65å…¬æ–¤ï¼Œèº«é«˜172å˜ç±³ã€‚è¡€å¸¸è§„ç™½ç»†èƒ6.2ï¼Œè¡€çº¢è›‹ç™½125ï¼Œè¡€å°æ¿180ã€‚è‚è‚¾åŠŸèƒ½æ­£å¸¸ã€‚åˆå¹¶é«˜è¡€å‹è¯ç‰©æ§åˆ¶è‰¯å¥½ã€‚æ— å…¶ä»–ä¸¥é‡ç–¾ç—…ã€‚æ‚£è€…æ„¿æ„é…åˆä¸´åºŠè¯•éªŒã€‚"],
                ["æ‚£è€…å¥³æ€§58å²ï¼ŒHER2é˜³æ€§ä¹³è…ºç™Œï¼Œå·²å®Œæˆæ–°è¾…åŠ©åŒ–ç–—ï¼Œè®¡åˆ’è¿›è¡Œæ‰‹æœ¯æ²»ç–—ã€‚å¿ƒåŠŸèƒ½æ­£å¸¸ï¼ŒLVEF 60%ã€‚æ— å…¶ä»–é‡å¤§åˆå¹¶ç—‡ã€‚"],
                ["æ‚£è€…ç”·æ€§55å²ï¼Œèƒƒè…ºç™Œï¼Œä¸´åºŠåˆ†æœŸIIIæœŸï¼ŒHER2é˜´æ€§ï¼Œè®¡åˆ’æ¥å—å›´æ‰‹æœ¯æœŸåŒ–ç–—ã€‚ECOGè¯„åˆ†1åˆ†ã€‚"],
            ],
            inputs=[patient_input],
        )
    
    return demo


# ================= ä¸»ç¨‹åº =================

if __name__ == "__main__":
    print("=" * 70)
    print("TrialRAG Web Interface")
    print("=" * 70)
    print("\nå¯åŠ¨ Gradio æœåŠ¡å™¨...")
    print("å¯åŠ¨åè¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ˜¾ç¤ºçš„ URL\n")
    
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,
        share=False,  # è®¾ä¸º True å¯ç”Ÿæˆå…¬ç½‘é“¾æ¥
        show_error=True
    )
