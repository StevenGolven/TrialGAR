# -*- coding: utf-8 -*-


import os
import re
import json
import random
from datetime import datetime
from typing import List, Dict, Tuple, Optional
from xml.etree import ElementTree as ET

# ====================== é…ç½® ======================

ROOT_DIR = r"D:\å®ä¹ \all-detail"
OUT_DIR = r"D:\å®ä¹ \TrialGPT-China\cleaned_out_V4"

SHARD_SIZE = 5000
SAMPLE_N = 50

# Word 2003 XML å‘½åç©ºé—´
NS = {
    'w': 'http://schemas.microsoft.com/office/word/2003/wordml',
    'o': 'urn:schemas-microsoft-com:office:office'
}

# ====================== Format-Aware DOM Parser ======================

class ParagraphMetadata:
    """æ®µè½å…ƒæ•°æ®ï¼šæ–‡æœ¬ + æ ¼å¼ç‰¹å¾"""
    def __init__(self, text: str, is_bold: bool = False, font_size: int = 0):
        self.text = text.strip()
        self.is_bold = is_bold
        self.font_size = font_size
        self.is_likely_header = is_bold and len(text.strip()) < 30
    
    def __repr__(self):
        bold_mark = "**" if self.is_bold else ""
        return f"{bold_mark}{self.text}{bold_mark}"


class FormatAwareDocParser:
    """
    æ ¼å¼æ„ŸçŸ¥çš„Word XMLè§£æå™¨
    æ ¸å¿ƒæ”¹è¿›ï¼šæå–åŠ ç²—ã€å­—å·ç­‰æ ¼å¼ç‰¹å¾ï¼Œç”¨ä½œè¯­ä¹‰ä»£ç†
    """
    
    def __init__(self, file_path: str):
        self.path = file_path
        self.raw_content = ""
        self.root = None
        self.paragraphs = []  # List[ParagraphMetadata]
        self.ns = NS
        
    def parse_dom(self) -> bool:
        """
        DOMæ ‘è§£æï¼ˆä¼˜å…ˆç­–ç•¥ï¼‰
        æå–æ®µè½æ–‡æœ¬ + æ ¼å¼ç‰¹å¾ï¼ˆåŠ ç²—ã€å­—å·ï¼‰
        """
        try:
            with open(self.path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            self.raw_content = content
            
            # é¢„å¤„ç†CDATA
            content = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', content, flags=re.DOTALL)
            
            # è§£æXMLæ ‘
            self.root = ET.fromstring(content)
            
            # éå†æ‰€æœ‰æ®µè½ (w:p)
            for p_elem in self.root.findall('.//w:p', self.ns):
                p_text = ""
                is_bold = False
                font_size = 0
                
                # éå†æ®µè½å†…çš„æ‰€æœ‰run (w:r)
                for r_elem in p_elem.findall('.//w:r', self.ns):
                    # æå–æ ¼å¼å±æ€§ (w:rPr - Run Properties)
                    rPr = r_elem.find('w:rPr', self.ns)
                    if rPr is not None:
                        # æ£€æŸ¥åŠ ç²— (w:b)
                        if rPr.find('w:b', self.ns) is not None:
                            is_bold = True
                        
                        # æ£€æŸ¥å­—å· (w:sz)
                        sz_elem = rPr.find('w:sz', self.ns)
                        if sz_elem is not None:
                            try:
                                # Word XMLå­—å·å•ä½æ˜¯åŠç‚¹ï¼ˆ1pt = 2å•ä½ï¼‰
                                font_size = max(font_size, int(sz_elem.get('{' + self.ns['w'] + '}val', 0)) // 2)
                            except:
                                pass
                    
                    # æå–æ–‡æœ¬ (w:t)
                    t_elem = r_elem.find('w:t', self.ns)
                    if t_elem is not None and t_elem.text:
                        p_text += t_elem.text
                
                full_text = p_text.strip()
                if full_text:
                    # åˆ›å»ºå¸¦æ ¼å¼çš„æ®µè½å¯¹è±¡
                    para = ParagraphMetadata(full_text, is_bold, font_size)
                    self.paragraphs.append(para)
            
            return len(self.paragraphs) > 0
            
        except ET.ParseError:
            return False
        except Exception:
            return False
    
    def parse_regex_fallback(self) -> bool:
        """
        æ­£åˆ™å›é€€ç­–ç•¥ï¼ˆå½“DOMè§£æå¤±è´¥æ—¶ï¼‰
        è®ºæ–‡å–ç‚¹ï¼šRobustness through hybrid approach
        """
        if not self.raw_content:
            try:
                with open(self.path, 'r', encoding='utf-8', errors='ignore') as f:
                    self.raw_content = f.read()
            except:
                return False
        
        # æå–æ‰€æœ‰<w:t>èŠ‚ç‚¹
        text_nodes = re.findall(r'<w:t[^>]*>(.*?)</w:t>', self.raw_content, re.DOTALL)
        
        texts = []
        for node in text_nodes:
            # å¤„ç†CDATA
            if '<![CDATA[' in node:
                cdata = re.findall(r'<!\[CDATA\[(.*?)\]\]>', node, re.DOTALL)
                texts.extend(cdata)
            else:
                clean = re.sub(r'<[^>]+>', '', node)
                if clean.strip():
                    texts.append(clean.strip())
        
        if len(texts) < 10:
            # æœ€åçš„å›é€€ï¼šç›´æ¥æå–CDATA
            cdata_blocks = re.findall(r'<!\[CDATA\[(.*?)\]\]>', self.raw_content, re.DOTALL)
            texts = cdata_blocks
        
        # è½¬æ¢ä¸ºParagraphMetadataï¼ˆä½†æ²¡æœ‰æ ¼å¼ä¿¡æ¯ï¼‰
        for text in texts:
            if text.strip():
                self.paragraphs.append(ParagraphMetadata(text.strip()))
        
        return len(self.paragraphs) > 0
    
    def parse(self) -> bool:
        """
        ç»Ÿä¸€è§£æå…¥å£
        å…ˆå°è¯•DOMï¼Œå¤±è´¥åˆ™å›é€€åˆ°æ­£åˆ™
        """
        # ç­–ç•¥1: DOMè§£æï¼ˆå¯ä»¥æå–æ ¼å¼ï¼‰
        if self.parse_dom():
            return True
        
        # ç­–ç•¥2: æ­£åˆ™å›é€€ï¼ˆæ— æ ¼å¼ä¿¡æ¯ï¼Œä½†æ›´é²æ£’ï¼‰
        return self.parse_regex_fallback()
    
    def get_paragraphs(self) -> List[ParagraphMetadata]:
        """è·å–æ‰€æœ‰æ®µè½ï¼ˆå¸¦æ ¼å¼ï¼‰"""
        return self.paragraphs
    
    def get_plain_text(self) -> str:
        """è·å–çº¯æ–‡æœ¬ï¼ˆå‘åå…¼å®¹ï¼‰"""
        return ' '.join([p.text for p in self.paragraphs])


# ====================== Format-Aware Field Extractor ======================

class StateBasedExtractor:
    """
    åŸºäºçŠ¶æ€æœºçš„å­—æ®µæå–å™¨
    æ ¸å¿ƒæ”¹è¿›ï¼šåˆ©ç”¨æ ¼å¼ç‰¹å¾ï¼ˆåŠ ç²—ï¼‰è¾…åŠ©sectionåˆ‡æ¢
    """
    
    def __init__(self, parser: FormatAwareDocParser):
        self.parser = parser
        self.paragraphs = parser.get_paragraphs()
        self.full_text = parser.get_plain_text()
    
    def extract_trial_id(self) -> str:
        """
        æå–Trial IDï¼ˆä¸‰é‡ç­–ç•¥ï¼‰
        """
        # ç­–ç•¥1: ä»æ–‡ä»¶å
        filename = os.path.basename(self.parser.path)
        m = re.search(r'(CTR\d{8})', filename)
        if m:
            return m.group(1)
        
        # ç­–ç•¥2: ä»æ–‡æœ¬
        m = re.search(r'(CTR\d{8})', self.full_text)
        if m:
            return m.group(1)
        
        # ç­–ç•¥3: ä»"ç™»è®°å·"é™„è¿‘
        for i, para in enumerate(self.paragraphs):
            if 'ç™»è®°å·' in para.text or 'æ³¨å†Œå·' in para.text:
                # æŸ¥çœ‹åç»­å‡ ä¸ªæ®µè½
                for j in range(i, min(i+5, len(self.paragraphs))):
                    m = re.search(r'(CTR\d{8})', self.paragraphs[j].text)
                    if m:
                        return m.group(1)
        
        return ""
    
    def _find_field_with_format(self, keywords: List[str]) -> str:
        """
        åˆ©ç”¨æ ¼å¼ç‰¹å¾è¾…åŠ©å­—æ®µæŸ¥æ‰¾
        å…³é”®æ”¹è¿›ï¼šå¦‚æœçœ‹åˆ°åŠ ç²—çš„çŸ­æ–‡æœ¬åŒ…å«å…³é”®è¯ï¼Œä¼˜å…ˆè®¤ä¸ºå®ƒæ˜¯æ ‡é¢˜
        """
        for i, para in enumerate(self.paragraphs):
            text = para.text
            
            # æ£€æŸ¥å…³é”®è¯
            for keyword in keywords:
                if keyword in text:
                    # å¦‚æœæ˜¯åŠ ç²—çš„çŸ­æ–‡æœ¬ï¼Œå¾ˆå¯èƒ½æ˜¯æ ‡é¢˜ï¼Œå–åç»­å†…å®¹
                    if para.is_likely_header:
                        # æ”¶é›†åç»­æ®µè½ç›´åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜
                        content = []
                        for j in range(i+1, min(i+15, len(self.paragraphs))):
                            next_para = self.paragraphs[j]
                            # é‡åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜å°±åœæ­¢
                            if next_para.is_likely_header:
                                break
                            content.append(next_para.text)
                        
                        result = ' '.join(content).strip()
                        if result:
                            return result
                    else:
                        # ä¸æ˜¯æ ‡é¢˜ï¼Œå¯èƒ½å…³é”®è¯å’Œå†…å®¹åœ¨åŒä¸€æ®µ
                        # å»æ‰å…³é”®è¯æœ¬èº«
                        result = text.replace(keyword, '', 1).strip()
                        if result:
                            return result
        
        return ""
    
    def extract_titles(self) -> Tuple[str, str]:
        """æå–è¯•éªŒé¢˜ç›®"""
        public = self._find_field_with_format(['è¯•éªŒé€šä¿—é¢˜ç›®', 'å…¬ç¤ºæ ‡é¢˜', 'é€šä¿—é¢˜ç›®'])
        scientific = self._find_field_with_format(['è¯•éªŒä¸“ä¸šé¢˜ç›®', 'ç§‘å­¦æ ‡é¢˜', 'ä¸“ä¸šé¢˜ç›®'])
        return public, scientific
    
    def extract_conditions(self) -> List[str]:
        """æå–é€‚åº”ç—‡"""
        text = self._find_field_with_format(['é€‚åº”ç—‡', 'ç–¾ç—…', 'ç—…ç—‡'])
        if not text:
            return []
        
        parts = re.split(r'[;ï¼›,ï¼Œ/ã€\n]+', text)
        conditions = []
        for p in parts:
            p = p.strip()
            if 1 < len(p) < 50:
                conditions.append(p)
        
        return conditions[:10]
    
    def extract_phase(self) -> str:
        """æå–è¯•éªŒåˆ†æœŸ"""
        text = self._find_field_with_format(['è¯•éªŒåˆ†æœŸ', 'ç ”ç©¶é˜¶æ®µ', 'ä¸´åºŠåˆ†æœŸ'])
        if not text:
            return ""
        
        phase_map = {
            'â… ': 'IæœŸ', 'â…¡': 'IIæœŸ', 'â…¢': 'IIIæœŸ', 'â…£': 'IVæœŸ',
            'IæœŸ': 'IæœŸ', 'IIæœŸ': 'IIæœŸ', 'IIIæœŸ': 'IIIæœŸ', 'IVæœŸ': 'IVæœŸ',
            'I/II': 'I/IIæœŸ', 'II/III': 'II/IIIæœŸ',
        }
        
        for key, val in phase_map.items():
            if key in text:
                return val
        
        return text[:20]
    
    def extract_interventions(self) -> List[str]:
        """æå–å¹²é¢„æªæ–½"""
        text = self._find_field_with_format(['è¯ç‰©åç§°', 'è¯•éªŒè¯ç‰©', 'å¹²é¢„æªæ–½', 'è¯ç‰©ä¿¡æ¯'])
        if not text:
            return []
        
        # å»å™ªéŸ³
        text = re.sub(r'(è”ç³»äºº|ç”µè¯|é‚®ç®±|åœ°å€|å•ä½).{0,30}', ' ', text)
        
        # æå–è¯ç‰©å
        tokens = re.findall(r'[A-Za-z0-9\-]{3,}|[\u4e00-\u9fff]{2,10}', text)
        
        bad_words = {'å¯¹ç…§', 'å¸¸è§„', 'æ ‡å‡†', 'æ²»ç–—', 'æ–¹æ¡ˆ', 'è¯ç‰©', 'ä¼ä¸š', 'å…¬ç¤º'}
        
        drugs = []
        for token in tokens:
            token = token.strip()
            if len(token) < 2 or token in bad_words:
                continue
            if token not in drugs:
                drugs.append(token)
        
        return drugs[:12]
    
    def extract_brief_summary(self) -> str:
        """æå–è¯•éªŒç›®çš„"""
        return self._find_field_with_format(['è¯•éªŒç›®çš„', 'ç ”ç©¶ç›®çš„', 'è¯•éªŒç®€ä»‹'])
    
    def extract_criteria_format_aware(self, criteria_type: str) -> List[str]:
        """
        æ ¼å¼æ„ŸçŸ¥çš„å…¥æ’æ ‡å‡†æå–
        
        æ ¸å¿ƒæ”¹è¿›ï¼š
        1. åˆ©ç”¨åŠ ç²—åˆ¤æ–­sectionæ ‡é¢˜
        2. çŠ¶æ€æœºé©±åŠ¨ï¼Œé¿å…è¯¯åˆ¤
        3. ç»“åˆç¼–å·ç‰¹å¾
        
        criteria_type: 'inclusion' or 'exclusion'
        """
        if criteria_type == 'inclusion':
            start_keywords = ['å…¥é€‰æ ‡å‡†', 'çº³å…¥æ ‡å‡†', 'å…¥ç»„æ ‡å‡†']
            end_keywords = ['æ’é™¤æ ‡å‡†', 'å‰”é™¤æ ‡å‡†', 'ä¸»è¦ç»“å±€', 'æ¬¡è¦ç»“å±€']
        else:
            start_keywords = ['æ’é™¤æ ‡å‡†', 'å‰”é™¤æ ‡å‡†']
            end_keywords = ['ä¸»è¦ç»“å±€', 'æ¬¡è¦ç»“å±€', 'ç ”ç©¶è€…', 'ç”³åŠè€…', 'ä¼¦ç†']
        
        criteria_items = []
        current_state = 'SEEKING'  # çŠ¶æ€ï¼šSEEKING -> IN_SECTION -> ENDED
        
        for i, para in enumerate(self.paragraphs):
            text = para.text
            clean_text = text.replace(' ', '')
            
            # === çŠ¶æ€1: å¯»æ‰¾sectionèµ·å§‹ ===
            if current_state == 'SEEKING':
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…èµ·å§‹å…³é”®è¯
                for keyword in start_keywords:
                    if keyword in clean_text:
                        # âœ… æ ¼å¼æ„ŸçŸ¥åˆ¤æ–­ï¼šåŠ ç²— OR çŸ­æ–‡æœ¬ OR ç¼–å·å¼€å¤´
                        is_section_header = (
                            para.is_likely_header or 
                            len(clean_text) < 15 or
                            re.match(r'^[\dä¸€äºŒä¸‰å››]+[\.ã€]', clean_text)
                        )
                        
                        if is_section_header:
                            current_state = 'IN_SECTION'
                            break  # æ‰¾åˆ°äº†ï¼Œè·³è¿‡æ ‡é¢˜æœ¬èº«
            
            # === çŠ¶æ€2: æ”¶é›†sectionå†…å®¹ ===
            elif current_state == 'IN_SECTION':
                # æ£€æŸ¥æ˜¯å¦é‡åˆ°ç»“æŸå…³é”®è¯
                for end_kw in end_keywords:
                    if end_kw in clean_text:
                        # âœ… æ ¼å¼æ„ŸçŸ¥ï¼šå¦‚æœæ˜¯åŠ ç²—æ ‡é¢˜ï¼Œç¡®è®¤ç»“æŸ
                        if para.is_likely_header or len(clean_text) < 15:
                            current_state = 'ENDED'
                            break
                
                if current_state == 'ENDED':
                    break
                
                # æ”¶é›†å†…å®¹ï¼ˆè¿‡æ»¤æ˜æ˜¾çš„å™ªéŸ³ï¼‰
                if len(text) >= 5:
                    if not any(noise in text for noise in ['è”ç³»äºº', 'ç”µè¯', 'é‚®ç®±', 'åŒ»é™¢åç§°']):
                        criteria_items.append(text)
        
        # æ™ºèƒ½æ‹†åˆ†ï¼šå¦‚æœæå–çš„æ¡ç›®å¤ªå°‘ï¼Œå¯èƒ½æ˜¯ç²˜åœ¨ä¸€èµ·çš„
        if len(criteria_items) == 0:
            # å›é€€ï¼šç”¨åŸæ¥çš„æ–¹æ³•
            return self._extract_criteria_fallback(criteria_type)
        elif len(criteria_items) == 1 and len(criteria_items[0]) > 500:
            # å•æ¡å¤ªé•¿ï¼Œéœ€è¦æ‹†åˆ†
            return self._split_long_criteria(criteria_items[0])
        
        return criteria_items[:30]
    
    def _extract_criteria_fallback(self, criteria_type: str) -> List[str]:
        """å›é€€æ–¹æ³•ï¼šåŸºäºå®Œæ•´æ–‡æœ¬çš„åŒºé—´æå–"""
        if criteria_type == 'inclusion':
            keywords = ['å…¥é€‰æ ‡å‡†', 'çº³å…¥æ ‡å‡†', 'å…¥ç»„æ ‡å‡†']
            end_kws = ['æ’é™¤æ ‡å‡†', 'å‰”é™¤æ ‡å‡†']
        else:
            keywords = ['æ’é™¤æ ‡å‡†', 'å‰”é™¤æ ‡å‡†']
            end_kws = ['ä¸»è¦ç»“å±€', 'æ¬¡è¦ç»“å±€', 'ç ”ç©¶è€…']
        
        criteria_text = ""
        for kw in keywords:
            if kw in self.full_text:
                start = self.full_text.index(kw) + len(kw)
                end = len(self.full_text)
                
                for end_kw in end_kws:
                    idx = self.full_text.find(end_kw, start)
                    if idx != -1 and idx < end:
                        end = idx
                
                criteria_text = self.full_text[start:end]
                break
        
        if not criteria_text:
            return []
        
        return self._split_long_criteria(criteria_text)
    
    def _split_long_criteria(self, text: str) -> List[str]:
        """æ™ºèƒ½æ‹†åˆ†é•¿æ–‡æœ¬"""
        items = []
        
        # ç»Ÿä¸€æ›¿æ¢ç¼–å·
        text_marked = text
        text_marked = re.sub(r'(\d+)\s*[.ã€\.ã€‚)ï¼‰]\s*', r'|||ITEM\1||| ', text_marked)
        text_marked = re.sub(r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³]', '|||ITEM|||', text_marked)
        text_marked = re.sub(r'[ï¼ˆ\(]\s*(\d+)\s*[ï¼‰\)]', r'|||ITEM\1|||', text_marked)
        
        parts = text_marked.split('|||ITEM')
        
        for part in parts:
            part = part.replace('|||', '').strip()
            part = re.sub(r'^\d+\s*', '', part)
            part = part.strip('ï¼›;ï¼Œ,ã€‚. \t\n')
            
            if len(part) < 5:
                continue
            
            # äºŒæ¬¡æ‹†åˆ†ï¼šå¦‚æœä»ç„¶å¾ˆé•¿
            if len(part) > 500:
                sub_parts = part.split('ï¼›')
                items.extend([p.strip() for p in sub_parts if len(p.strip()) > 10])
            else:
                items.append(part)
        
        # å»é‡
        seen = set()
        result = []
        for item in items:
            if item and item not in seen:
                seen.add(item)
                result.append(item)
        
        return result[:30]


# ====================== Record Builder ======================

def build_record(parser: FormatAwareDocParser) -> Dict:
    """æ„å»ºæ ‡å‡†åŒ–è®°å½•"""
    extractor = StateBasedExtractor(parser)
    
    trial_id = extractor.extract_trial_id()
    public_title, scientific_title = extractor.extract_titles()
    
    record = {
        "trial_id": trial_id,
        "background": {
            "public_title": public_title,
            "scientific_title": scientific_title,
            "conditions": extractor.extract_conditions(),
            "interventions": extractor.extract_interventions(),
            "brief_summary": extractor.extract_brief_summary(),
        },
        "criteria": {
            "inclusion": extractor.extract_criteria_format_aware('inclusion'),
            "exclusion": extractor.extract_criteria_format_aware('exclusion'),
        },
        "meta": {
            "phase": extractor.extract_phase(),
            "raw_file": os.path.basename(parser.path),
            "parser_mode": "DOM" if parser.root is not None else "REGEX_FALLBACK"
        }
    }
    
    return record


# ====================== Quality Assessment ======================

def assess_quality_v4(record: Dict) -> Tuple[bool, List[str]]:
    """V4è´¨é‡è¯„ä¼°ï¼ˆä¸V3ç›¸åŒï¼Œä¿æŒä¸€è‡´æ€§ï¼‰"""
    issues = []
    
    trial_id = record.get("trial_id", "").strip()
    bg = record.get("background", {})
    cr = record.get("criteria", {})
    
    # ç¡¬æ€§è¦æ±‚ï¼štrial_id
    if not trial_id or not trial_id.startswith("CTR"):
        issues.append("missing_or_invalid_trial_id")
        return False, issues
    
    # è‡³å°‘æœ‰èƒŒæ™¯ä¿¡æ¯
    has_any_bg = any([
        bg.get("public_title"),
        bg.get("scientific_title"),
        bg.get("conditions"),
        bg.get("brief_summary")
    ])
    
    if not has_any_bg:
        issues.append("no_background_info")
    
    # è‡³å°‘æœ‰å…¥æ’æ ‡å‡†
    inc_count = len(cr.get("inclusion", []))
    exc_count = len(cr.get("exclusion", []))
    
    if inc_count == 0 and exc_count == 0:
        issues.append("no_eligibility_criteria")
    
    # è­¦å‘Š
    if inc_count == 0:
        issues.append("warning_no_inclusion")
    if exc_count == 0:
        issues.append("warning_no_exclusion")
    
    # Cleanåˆ¤å®š
    is_clean = not any(issue.startswith("missing") or issue.startswith("no_") 
                       for issue in issues if "warning" not in issue)
    
    return is_clean, issues


# ====================== Main Processing ======================

def process_file(file_path: str) -> Tuple[Optional[Dict], List[str]]:
    """å¤„ç†å•ä¸ªæ–‡ä»¶"""
    try:
        parser = FormatAwareDocParser(file_path)
        
        if not parser.parse():
            return None, ["parse_failed"]
        
        record = build_record(parser)
        
        return record, []
        
    except Exception as e:
        return None, [f"error: {str(e)}"]


def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)


def main():
    print("=" * 80)
    print("TrialGPT-China æ•°æ®æ¸…æ´—è„šæœ¬ - V4 æ ¼å¼æ„ŸçŸ¥ç‰ˆ")
    print("Structure + Format Aware Parsing (SIGIR-Ready)")
    print("=" * 80)
    
    # å‡†å¤‡ç›®å½•
    ensure_dir(OUT_DIR)
    clean_dir = os.path.join(OUT_DIR, "clean_parts")
    dirty_dir = os.path.join(OUT_DIR, "dirty")
    ensure_dir(clean_dir)
    ensure_dir(dirty_dir)
    
    # è¾“å‡ºæ–‡ä»¶
    dirty_path = os.path.join(dirty_dir, "trials_dirty.jsonl")
    failed_path = os.path.join(OUT_DIR, "failed_files.txt")
    report_path = os.path.join(OUT_DIR, "clean_report.json")
    sample_path = os.path.join(OUT_DIR, "sample_clean.jsonl")
    
    # ç»Ÿè®¡
    stats = {
        "total_files": 0,
        "parsed_ok": 0,
        "clean_count": 0,
        "dirty_count": 0,
        "fail_count": 0,
        "dom_mode_count": 0,
        "regex_mode_count": 0,
        "quality_issues": {},
    }
    
    sample_records = []
    
    # åˆ†ç‰‡ç®¡ç†
    shard_idx = 0
    shard_count = 0
    shard_file = None
    
    def open_new_shard():
        nonlocal shard_idx, shard_count, shard_file
        if shard_file:
            shard_file.close()
        shard_path = os.path.join(clean_dir, f"trials_clean_part_{shard_idx:03d}.jsonl")
        shard_file = open(shard_path, "w", encoding="utf-8")
        shard_count = 0
        shard_idx += 1
    
    open_new_shard()
    
    with open(dirty_path, "w", encoding="utf-8") as f_dirty, \
         open(failed_path, "w", encoding="utf-8") as f_failed:
        
        # éå†æ–‡ä»¶
        for root, _, files in os.walk(ROOT_DIR):
            for fname in files:
                if not fname.lower().endswith(".doc"):
                    continue
                
                stats["total_files"] += 1
                file_path = os.path.join(root, fname)
                
                # è¿›åº¦
                if stats["total_files"] % 500 == 0:
                    current_rate = (stats["clean_count"] / stats["total_files"] * 100) if stats["total_files"] > 0 else 0
                    dom_rate = (stats["dom_mode_count"] / stats["parsed_ok"] * 100) if stats["parsed_ok"] > 0 else 0
                    print(f"[è¿›åº¦] å·²å¤„ç†: {stats['total_files']} | "
                          f"Clean: {stats['clean_count']} ({current_rate:.1f}%) | "
                          f"DOMæ¨¡å¼: {stats['dom_mode_count']} ({dom_rate:.1f}%)")
                
                # å¤„ç†
                record, errors = process_file(file_path)
                
                if record is None:
                    stats["fail_count"] += 1
                    f_failed.write(f"{file_path}\t{';'.join(errors)}\n")
                    continue
                
                stats["parsed_ok"] += 1
                
                # ç»Ÿè®¡è§£ææ¨¡å¼
                parser_mode = record.get("meta", {}).get("parser_mode", "UNKNOWN")
                if parser_mode == "DOM":
                    stats["dom_mode_count"] += 1
                else:
                    stats["regex_mode_count"] += 1
                
                # è´¨é‡è¯„ä¼°
                is_clean, issues = assess_quality_v4(record)
                
                if is_clean:
                    # Cleanæ•°æ®
                    shard_file.write(json.dumps(record, ensure_ascii=False) + "\n")
                    stats["clean_count"] += 1
                    shard_count += 1
                    
                    # æŠ½æ ·
                    if len(sample_records) < SAMPLE_N and random.random() < 0.03:
                        sample_records.append(record)
                    
                    # åˆ†ç‰‡
                    if shard_count >= SHARD_SIZE:
                        open_new_shard()
                else:
                    # Dirtyæ•°æ®
                    record["_quality_issues"] = issues
                    f_dirty.write(json.dumps(record, ensure_ascii=False) + "\n")
                    stats["dirty_count"] += 1
                    
                    for issue in issues:
                        stats["quality_issues"][issue] = stats["quality_issues"].get(issue, 0) + 1
    
    # å…³é—­æ–‡ä»¶
    if shard_file:
        shard_file.close()
    
    # æ ·æœ¬
    with open(sample_path, "w", encoding="utf-8") as f_sample:
        for rec in sample_records:
            f_sample.write(json.dumps(rec, ensure_ascii=False) + "\n")
    
    # æŠ¥å‘Š
    report = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "version": "V4 (Structure + Format Aware)",
        "config": {
            "root_dir": ROOT_DIR,
            "out_dir": OUT_DIR,
            "shard_size": SHARD_SIZE,
        },
        "statistics": stats,
        "clean_rate": stats["clean_count"] / stats["total_files"] if stats["total_files"] > 0 else 0,
        "parse_success_rate": stats["parsed_ok"] / stats["total_files"] if stats["total_files"] > 0 else 0,
        "dom_success_rate": stats["dom_mode_count"] / stats["parsed_ok"] if stats["parsed_ok"] > 0 else 0,
        "notes": "V4: Structure-Aware (DOM) + Format-Aware (Bold/FontSize) + State Machine"
    }
    
    with open(report_path, "w", encoding="utf-8") as f_report:
        json.dump(report, f_report, ensure_ascii=False, indent=2)
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("âœ… æ•°æ®æ¸…æ´—å®Œæˆï¼")
    print("=" * 80)
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"  - è§£ææˆåŠŸ: {stats['parsed_ok']} ({report['parse_success_rate']:.1%})")
    print(f"  - Cleanæ•°æ®: {stats['clean_count']} ({report['clean_rate']:.1%})")
    print(f"  - Dirtyæ•°æ®: {stats['dirty_count']}")
    print(f"  - å¤±è´¥æ–‡ä»¶: {stats['fail_count']}")
    print(f"\nğŸ” è§£ææ¨¡å¼ç»Ÿè®¡:")
    print(f"  - DOMæ¨¡å¼: {stats['dom_mode_count']} ({report['dom_success_rate']:.1%})")
    print(f"  - REGEXå›é€€: {stats['regex_mode_count']}")
    
    if stats['quality_issues']:
        print(f"\nğŸ“‹ ä¸»è¦è´¨é‡é—®é¢˜:")
        for issue, count in sorted(stats['quality_issues'].items(), key=lambda x: -x[1])[:5]:
            print(f"  - {issue}: {count}")
    
    print(f"\nğŸ“ è¾“å‡ºç›®å½•:")
    print(f"  - Cleanåˆ†ç‰‡: {clean_dir}")
    print(f"  - Dirtyæ•°æ®: {dirty_dir}")
    print(f"  - ç»Ÿè®¡æŠ¥å‘Š: {report_path}")
    print(f"  - æ ·æœ¬æ–‡ä»¶: {sample_path}")
    print("=" * 80)


if __name__ == "__main__":
    main()
